\documentclass[base=hide,12pt]{elegantbook}
% Para Linux
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}

\title{Unidad : Regresion lineal}
\subtitle{Probabilidad y Estadística}

\author{Daniel Enrique González Gómez}
\institute{Pontificia Universidad Javeriana Cali}
\date{Mayo, 2020}
\version{1.00}
\bioinfo{Area}{Estadística}

% Frase....
% \extrainfo{}

%\logo{logo-blue.png}
\cover{banner_o3.png}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{color}
\usepackage{tcolorbox}
%%\usepackage[margin=0.5in]{geometry}
%\usepackage{amsthm,amssymb,amsfonts}
%%\usepackage{tikz,lipsum,lmodern}
%\usepackage[most]{tcolorbox}
%\usepackage{xcolor}

\definecolor{col1}{rgb}{0.42,0.35,0.80}% magenta 
\definecolor{col2}{rgb}{0.0,0.65,0.31}%   verde
\definecolor{col3}{rgb}{1.0,0.49,0.09}%   naranja
\definecolor{col4}{rgb}{0.0,0.2,0.6}%  azul oscuro 
\definecolor{col5}{rgb}{0.99,0.05,0.21}%  rojo

%---------------------------------------------------------------------------------------------
\newtcolorbox{Box1}[2][]{	   % caja  azul
	colback=white!95!col1,
	colframe=white!20!col1,	fonttitle=\bfseries,
	colbacktitle=white!10!col1,enhanced,
	attach boxed title to top left={xshift=1cm,	yshift=-2mm},
	title=#2,#1}
%------------------------------------------------------ --------------------------------------
\newtcolorbox{Box2}[2][]{  % caja verde  ok
	colback=white!95!col2,
	colframe=white!20!col2,	fonttitle=\bfseries,
	colbacktitle=white!10!col2,enhanced,
	attach boxed title to top left={xshift=1cm,	yshift=-2mm},
	title=#2,#1}
%-------------------------------------------------------------------------------------------

\newtcolorbox{Box3}[2][]{ % caja naranja
	colback=white!95!col3,
	colframe=white!20!col3,	fonttitle=\bfseries,
	colbacktitle=white!10!col3,enhanced,
	attach boxed title to top left={xshift=1cm,	yshift=-2mm},
	title=#2,#1}

%----------------------------------------------------------------------------------------

\newtcolorbox{Box4}[2][]{  % caja purpura  ok
	colback=white!95!col4,
	colframe=white!20!col4,	fonttitle=\bfseries,
	colbacktitle=white!10!col4,enhanced,
	attach boxed title to top left={xshift=1cm,	yshift=-2mm},
	title=#2,#1}

%----------------------------------------------------------------------------------------
\newtcolorbox{Box5}[2][]{    %
	colback=white!95!col5,
	colframe=white!20!col5,	fonttitle=\bfseries,
	colbacktitle=white!10!col5,enhanced,
	attach boxed title to top left={xshift=1cm,	yshift=-2mm},
	title=#2,#1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtcolorbox{mybox}[2][]{boxsep=1em,left=-0em,
	
	colback=blue!5!white, 
	colframe=blue!75!black, 
	fonttitle=\bfseries\sffamily,
	colbacktitle=blue!85!red!60,enhanced,
	
	attach boxed title to top left={yshift=-3mm,xshift=3mm},
	title=#2,#1}


\newtcolorbox{mybox2}[2][]{%
	colback=bg,
	colframe=blue!75!black,	fonttitle=\bfseries,
	coltitle=blue!75!black,
	colbacktitle=white!5!col5,enhanced,
	attach boxed title to top left={yshift=-1.2mm, xshift=2mm},
	title=#2,#1}
%-----------------------------------------------------------------------------------------
\font\domino=domino
\def\die#1{{\domino#1}}



\setlength{\parindent}{0cm}
\spanishdecimal{.} % punto decimal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\textcolor{col4}{\LARGE \bf Modelos especiales de probabilidad}    \\
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\textcolor{col4}{\LARGE  \bf Distribución Bernoulli}\\
	%\begin{Box2}{Propiedades}
	%Propiedades
	%\end{Box2}
	%\begin{Box4}{Titulo}
	%	Definiciones
	%\end{Box4}
	%%ejemplo 1 ================================================================================
	%\textcolor{col3}{\bf Ejemplo 1.}  \\
	%%====================================================================================
	%\textcolor{col4}{\bf Solución: }\\
	%\textcolor{col1}{\bf Ejemplo 9:}\\
	%\textcolor{col5}{\bf Nota:}\\
	%\begin{lstlisting}
	%codigo R
	%\end{lstlisting}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\textcolor{col4}{\LARGE \bf Modelo de Regresión lineal}    \\
	
	En esta unidad estudiaremos la relación que puede existir entre dos variables o una variable y un conjunto de variables, mediante la construcción de un modelo lineal que represente dicha relación. Se partirá de un modelo general (modelo de Regresión Lineal Múltiple) y se tratará como caso particular el modelo de dos variables también llamado Modelo de Regresión Lineal Simple MRLS.\\
	
	En unidades anteriores se estudiaron relaciones entre dos variables aleatorias, las cuales se puede medir a través del coeficiente de correlación ($\rho$),  el cual puede presentar valores desde  -1  hasta 1 como se representa en las siguientes graficas :
	
	
	\begin{tabular}{ccc}
		\includegraphics[scale=.3]{Grafico1.pdf}&
		\includegraphics[scale=.3]{Grafico2.pdf}&
		\includegraphics[scale=.3]{Grafico3.pdf} \\
	\end{tabular}
	\begin{tabular}{ccc}
		\includegraphics[scale=.3]{Grafico4.pdf} &
		\includegraphics[scale=.3]{Grafico5.pdf}&
		\includegraphics[scale=.3]{Grafico6.pdf}\\
		\end{tabular}
\begin{tabular}{ccc}	
		\includegraphics[scale=.3]{Grafico7.pdf}&
		\includegraphics[scale=.3]{Grafico8.pdf}&
		\includegraphics[scale=.3]{Grafico9.pdf}\\
	\end{tabular}
\begin{tabular}{ccc}		
		\includegraphics[scale=.3]{Grafico10.pdf}&
		\includegraphics[scale=.3]{Grafico11.pdf}&
		\includegraphics[scale=.3]{Grafico12.pdf}\\
	\end{tabular}
	
	La primera grafica (primera fila y primera columna) y la ultima grafica (cuarta fila y tercera columna) representan relaciones perfectas entre dos variables, que tan solo se presentan cuando la relación es constante $Y = a + bX$.  A medida que el coeficiente de correlación aumenta la nube de puntos cambia, permitiendo ver una gama de posibilidades que pueden ser interpretadas dependiendo el signo como relaciones negativas o positivas (que se evidencian en la pendiente $\beta_{0}$) del modelo y por la magnitud desde relaciones muy fuertes a relaciones muy débiles (cerca de cero)
	
	
	El modelo de regresión poblacional está determinado por:
	
	\begin{Box2}{}
	\begin{equation*}
	Y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+ \dots + \beta_{k}X_{k} +u
	\end{equation*}
	\end{Box2}
	
	
	Donde:
	\begin{itemize}
		\item $Y$,representa una la variable dependiente, también llamada variable respuesta o variable predicha.
		\item $X_{1},X_{2},\dots, X_{k}$, corresponden a un conjunto de variables independientes o predictoras.
		\item $\beta_{0},\beta_{1},\beta_{2},\dots, \beta{k}$ son un conjunto de constantes o parámetros del modelo (constantes) que deben ser estimadas a partir del conjunto de valores obtenidos en una muestra.
		\item $u$ corresponde a una variable aleatoria no observable que representa todas las variables que no se incluyen en el modelo y corresponde a un componente aleatorio producto del azar de la naturaleza. Sin la presencia de esta variable en la estructura de la relación, el modelo formulado corresponderá a una relación matemática.
	\end{itemize}

Un caso particular del modelo, ocurre cuando solo tiene una variable independiente, en este caso el modelo toma el nombre de Modelo de Regresión Lineal Simple con la siguiente estructura:
	
		\begin{Box2}{}
		\begin{equation*}
		Y=\beta_{0} + \beta_{1} X + u
		\end{equation*}
		\end{Box2}
	
En este caso es posible realzar una representación gráfica del modelo mediante una linea recta donde $\beta_{0}$ corresponde al intercepto y $\beta_{1}$  a la pendiente. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A partir de la obtención de una muestra de tamaño $n$ se puede obtener el modelo estimado, cuya ecuación esta dada por:
\begin{Box2}{}
	\begin{equation*}
	\widehat{E[y|X]}=b_{0}+b_{1}x_{1}+b_{2}x_{2}+\dots+b_{k}x_{k}
	\end{equation*}
\end{Box2}

Donde : $[b_{0}, b_{1}, \dots, b_{k}]$ representa el vector de estimadores de los coeficientes del modelo. \\

Como cada vez que se toma una muestra, por razones del azar los objetos medidos son diferentes, es claro que al realizar un procedimiento de estimación los valores de estos son diferentes, ratificando esto que $\widehat{\beta}_{0},\widehat{\beta}_{1},\widehat{\beta}_{2},\dots, \widehat{\beta}_{k}$  son variables aleatorias \\


\textcolor{col4}{\Large \bf Modelo de Regresión lineal - enfoque matricial}    \\

Para representar el modelo de manera matricial, se puede partir de la siguiente ecuación:
\begin{Box2}{}
	\begin{equation*}
	Y_{i}=\beta_{0}+\beta_{1}X_{1i}+\beta_{2}X_{2i}+ \dots + \beta_{k}X_{ki} +u_{i} \hspace{.3cm} i=1,2,\dots,n
	\end{equation*}
\end{Box2}
	
	Los valores $\beta_{0},\beta_{1},\dots,\beta_{k}$ deben cumplir las siguientes igualdades:
\begin{center}
		\begin{eqnarray*}
		y_{1}&=&\beta_{0}+\beta_{1}x_{11}+\beta_{2}x_{21}+\dots+\beta_{k}x_{k1}+u_{1} \\
		y_{2}&=&\beta_{0}+\beta_{1}x_{12}+\beta_{2}x_{22}+\dots+\beta_{k}x_{k2}+u_{2} \\
		y_{3}&=&\beta_{0}+\beta_{1}x_{13}+\beta_{2}x_{23}+\dots+\beta_{k}x_{k3}+u_{3} \\
		\vdots && \vdots \hspace{1cm} \vdots \hspace{1cm} \vdots \\
		y_{n}&=&\beta_{0}+\beta_{1}x_{1n}+\beta_{2}x_{2n}+\dots+\beta_{k}x_{kn}+u_{n} \\
	\end{eqnarray*}
\end{center}

	En forma matricial las podemos escribir
\begin{Box2}{}	
	\begin{equation*}
	y = X \beta + u 
	\end{equation*}
\end{Box2}

	Donde: 
	
	$$y = [y_{1},y_{2}, \dots, y_{n}]^{T}$$ 
	
	$$\beta =[\beta_{0},\beta_{1},\dots, \beta_{k}]^{T}$$
	
	$$X= \begin{bmatrix}
	{1}   &{x_{11}}&{x_{21}}& \dots & x_{k1}  \\
	{1}   &{x_{12}}&{x_{22}}& \dots & x_{k2}  \\
	{1}   &{x_{13}}&{x_{23}}& \dots & x_{k3}  \\
	\dots & \dots  &\dots   & \dots & \dots \\
	{1}   &{x_{1n}}&{x_{2n}}& \dots & x_{kn} \\
	\end{bmatrix}$$
	
	$$u =[u_{1},u_{2},\dots,u_{n}]^{T}$$   \\ \\
	
	
	\textcolor{col4}{\Large \bf Estimación de los coeficientes del modelo} 	
	\\
	
El método Mínimos Cuadrados Ordinarios (MCO) se basa en la minimización de la suma de los residuales ($\widehat{u}$), los cuales se pueden despejar de la ecuación del modelo $\widehat{y} = X \widehat{\beta} + \widehat{u}$. Como los errores son variables no observables, el método hace uso de sus respectivos estimadores llamados residuales:
\begin{Box2}{}
	\begin{equation*}
	\widehat{u} = \widehat{y} - X\widehat{\beta}
	\end{equation*}
\end{Box2}
	
Ahora, la suma de cuadrado de los residuales  se puede obtener al multiplicar el vector $\widehat{u}^{T} \widehat{u}$
 	
	\begin{equation*}
	SCRes=\sum_{i=1}^{n} \widehat{u}^{2}_{i} = \widehat{u}^{T} \widehat{u}=
	\begin{bmatrix}{\widehat{u}_{1}}\\
	{\widehat{u}_{2}}\\
	{\vdots}\\
	{\widehat{u}_{n}}\\
	\end{bmatrix}[\widehat{u}_{1},\widehat{u}_{2},\dots,\widehat{u}_{n}] 
	\end{equation*}
	
	\begin{eqnarray*}
		SCRes&=&[\widehat{y}-Xb]^{T}[\widehat{y}-Xb]\\
		&=& y^{T}y -y^{T}Xb-b^{T}X^{T}y+b^{T}X^{T}Xb \\
		&=& y^{T}y -2bX^{T}y+b^{T}X^{T}Xb \\ 
	\end{eqnarray*}
	Para encontrar los valores óptimos de los coeficientes, se debe derivar parcialmente con respecto al vector $\beta$ e igualarlo a cero
	
	\begin{equation*}
	\dfrac{\partial{SCRes}}{\partial{\beta}}= -2X^{T}+ 2X^{T}Xb=0
	\end{equation*}
	
	De la ecuación anterior se despeja $b$ ($\widehat{\beta}$)

	\begin{eqnarray*}
		2X^{T} &=& 2X^{T}Xb\\
		(X^{T}X)^{-1}X^{T}y &=&(X^{T}X)^{-1}X^{T}X b\\
		(X^{T}X)^{-1}X^{T}y&=& b
	\end{eqnarray*}

	El estimador MCO de los coeficientes $b$ es:

\begin{Box2}{}
	\begin{equation*}
	\widehat{\beta}_{_{MCO}}= b = (X^{T}X)^{-1}X^{T}y
	\end{equation*}
\end{Box2}
	
Es claro que para poder realizar este proceso es condición necesaria que la matriz $(X^{T}X)$ sea invertible. En caso contrario los estimadores MCO no se pueden hallar.
	La versión del método MCO a partir de sumatorias se encuentra desarrollada en el texto guía (Walpole-2012 pp.395) \\
	
Para que la estimación por MCO sea valida y utilizada, se deben verificar que se cumplan los supuestos sobre los errores $u$ y las pruebas de significancia global y unitarias\\


\begin{itemize}
	\item[$S_1$:] El valor esperado de $u$ es cero.$E[u]=0$\vspace{.3cm} $E[u]=0$
	\item[$S_2$:] Los errores tienen varianza constante.\hspace{.3cm} $V[u_{i}]=\sigma^{2}$
	\item[$S_3$:] $u$ es una variable con distribución normal.\hspace{.3cm} $u \sim$ $Normal$
	\item[$S_4$:] Los errores son independientes unos de otros\hspace{.3cm} $E[u_i,u_j]$ que se entiende como la no autocorrelacion de los errores 
	\item[$S_5$:] Para el caso de regresión lineal múltiple se incluye el supuesto de no multicolinealidad. En caso de que se presenta multicolinealidad entre dos variables independientes da como resultado la  imposibilidad de poder realizar la estimaciones mediante MCO
\end{itemize}
	





	
	
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textcolor{col3}{\bf \large Ejemplo 1:} Se quiere determinar si existe relación entre los ingresos que recibe una familia y el valor de su consumo. Con en este propósito se toma información de 12 hogares, \\

\begin{center}
	\begin{tabular}{l|rrrrrrrrrrrr} 
x & 24.3 & 12.5 & 31.2 & 28.0 & 35.1 &10.5 & 23.2 & 10.0 & 8.5 & 15.9 & 14.7 &15.0\\
\hline 
y & 16.2 &  8.5 & 15.0 & 17.0 & 24.2 &11.2 & 15.0 &  7.1 & 3.5 & 11.5 & 10.7 &9.2\\
\end{tabular}
\end{center}

El primer paso corresponde a construir una grafica de dispersión, con el fin de poder visualizar el tipo de relación que se puede percibir entre las dos variables

\begin{Box3}{Código R}
\begin{verbatim} 	
x=c(24.3,12.5,31.2,28,35.1,10.5,23.2,10,8.5,15.9,14.7,15)
y=c(16.2,8.5,15,17,24.2,11.2,15,7.1,3.5,11.5,10.7,9.2)

plot(x,y, xlab = "Ingresos", ylab = "Consumo",
pch=21, bg="blue", xlim=c(0,35), ylim=c(0,25))
\end{verbatim}
\end{Box3}	
\includegraphics[scale=.8]{g1_ejemplo1}

En la grafica se puede observar una nube de  puntos que sugieren una relación positiva entre las dos  variables.  El objetivo entonces es determinar una ecuación que permita estimar esta relación que en este caso se representa por una linea recta.\\


\includegraphics[scale=.8]{g2_ejemplo1}
Esta recta representa la relación lineal entre  las dos variables, en este caso tiene una pendiente positiva que mide la magnitud del cambio en la variable dependiente ante un cambio unitario en la variable independiente.

Para encontrar la los valores estimados del intercepto y de la pendiente, utilizaremos el método de mínimos cuadrados ordinario (MCO) 

\begin{Box3}{}
	\begin{verbatim}
regresion=lm(y ~ x)
summary(regresion)

Call:
lm(formula = y ~ x)

Residuals:
Min      1Q  Median      3Q     Max 
-4.1928 -0.5426  0.0088  0.8500  3.5613 

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.77788    1.58292   1.123    0.288    
x            0.55817    0.07567   7.376 2.38e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.251 on 10 degrees of freedom
Multiple R-squared:  0.8447,	Adjusted R-squared:  0.8292 
F-statistic: 54.41 on 1 and 10 DF,  p-value: 2.38e-05
\end{verbatim}
\end{Box3}


De acuerdo con estos resultados obtenidos en R, muestra que la ecuación estimada es :

$$\widehat{y} = 1.77788 + 0.55817$$


Para validar estos resultados, para poder ser utilizados se debe:
\begin{enumerate}
	\item Validar que se cumplen los supuestos sobre $u$
	\item Verificar en los valores de los coeficientes mediante las pruebas de hipótesis individuales
\end{enumerate}

\vspace{.5cm}
\textcolor{col3}{\bf \large Validación de los supuestos}\\

Los supuestos a validar se deben verificar sobre los residuales $\widehat{u_{i}}$ que corresponde a la diferencia entre $\widehat{y_{i}}$ y $y$ y que son las realizaciones de los errores.

\begin{Box3}{}
\begin{verbatim}
yhat=predict(regresion)

 data.frame(x,y,yhat, residuales)
x    y      yhat residuales
1  24.3 16.2 15.341446  0.8585544
2  12.5  8.5  8.755023 -0.2550230
3  31.2 15.0 19.192828 -4.1928284
4  28.0 17.0 17.406680 -0.4066799
5  35.1 24.2 21.369697  2.8303031
6  10.5 11.2  7.638680  3.5613199
7  23.2 15.0 14.727457  0.2725429
8  10.0  7.1  7.359594 -0.2595944
9   8.5  3.5  6.522337 -3.0223373
10 15.9 11.5 10.652806  0.8471942
11 14.7 10.7  9.983000  0.7169999
12 15.0  9.2 10.150451 -0.9504515
\end{verbatim}	
\end{Box3}
\vspace{1cm}
\textcolor{col3}{\bf $S_1$: El valor esperado de los errores es cero $E[u]=0$}\\

\begin{Box3}{}
\begin{verbatim}
	summary(residuos)
 Min.        1st Qu.    Median     Mean       3rd Qu.    Max. 
-2.150513   -0.260479   0.003096   0.002784   0.398056   1.733197 

t.test(residuos)

One Sample t-test

data:  residuos
t = 0.0088183, df = 11, p-value = 0.9931
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
-0.6920463  0.6976140
sample estimates:
mean of x 
0.002783854 
	
\end{verbatim}
\end{Box3}
\vspace{1cm}

\textcolor{col3}{\bf $S_2$: Los errores tienen varianza constante $V[u]=\sigma^{2}$. Homoscedasticidad}\\

\begin{Box3}{}
\begin{verbatim}	
# Prueba de Goldfeld-Quandt
# Ho no existe heteroscedasticidad
gqtest(y~x)


Goldfeld-Quandt test

data:  y ~ x
GQ = 0.19285, df1 = 4, df2 = 4, p-value = 0.93
alternative hypothesis: variance increases from segment 1 to 2
\end{verbatim}
\end{Box3}
\vspace{1cm}

\textcolor{col3}{\bf $S_3$: Los errores se distribuyen normal $u\sim$ $N(0,\sigma^{2})$}\\
\begin{Box3}{}
\begin{verbatim}
	shapiro.test(residuos)
	
	Shapiro-Wilk normality test
	
	data:  residuos
	W = 0.93194, p-value = 0.4011
\end{verbatim}	
	
\end{Box3}
\vspace{1cm}

\textcolor{col3}{\bf $S_4$: Los errores son independientes $E[u_i,u_j]=0$}\\
\begin{Box3}{}
\begin{verbatim}
	> dwtest(y~x)
	
	Durbin-Watson test
	
	data:  y ~ x
	DW = 1.5517, p-value = 0.1769
	alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}	
	
\end{Box3}
\vspace{1cm}
\textcolor{col3}{\bf \large Pruebas de significancia individuales}\\
Después de verificar el cumplimiento de los supuestos sobre los errores se pasa a analizar los resultados de la estimación.

Las pruebas a realizar corresponden a la validación si los resultados obtenidos sobre el intercepto y la pendiente son significativamente diferentes de cero:\\




\begin{Box3}{}
\begin{verbatim}
Coefficients:
             Estimate   Std. Error   t value   Pr(>|t|)    
(Intercept)  1.77788    1.58292      1.123     0.288    
x            0.55817    0.07567      7.376     2.38e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

\end{verbatim}	
\end{Box3}	

\begin{tabular}{p{4cm}p{4cm}}
\begin{tabular}{c}
$Ho: \beta_{0} = 0$\\
$Ha: \beta_{0} \neq 0$\\
$EdeP:$ $t=1,123$\\
$valor-p:$ $0.288$\\
\end{tabular}
&
\begin{tabular}{c}
$Ho: \beta_{1} = 0$\\
$Ha: \beta_{1} \neq 0$ \\
$EdeP :$ $t=7.376$\\
$valor-p:$ $0.00005$
\end{tabular}\\
\end{tabular}


\vspace{.5cm}
\textcolor{col3}{\bf \large Analisis de varianza - ANOVA}\\

ANOVA corresponde a una prueba de hipotesis conjunta sobre todos las pendienes del modelo de manera  conjunta:


$Ho: \beta_{1} = \beta_{2} = ....= \beta_{k} = 0$\\
$Ha: \beta_{j}$ es diferente de cero\\

Esta prueba se fundamenta en la comparacion de dos varianza : La varianza relacionada con el modelo ($\sigma^{2}$) y la varianza relacionada con los errores ($\sigma^{2}_{u}$).


\begin{Box3}{}
	\begin{verbatim}
		> anova(regresion)
		Analysis of Variance Table
		
		Response: y
           Df   Sum Sq    Mean Sq    F value     Pr(>F)    
x           1   275.590   275.590    54.408      2.38e-05 ***
Residuals  10    50.652     5.065                     
		---
		
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.251 on 10 degrees of freedom
Multiple R-squared:  0.8447,	Adjusted R-squared:  0.8292 
F-statistic: 54.41 on 1 and 10 DF,  p-value: 2.38e-05		
\end{verbatim}
\end{Box3}


Ahora podemos realizar la interpretación de los resultados

$$\widehat{y} = 1.77788 + 0.55817 x$$

$\widehat{\beta_{0}} = 1.7788$, se interpreta como la parte del consumo que no depende de los ingresos al cual se le llama consumo autónomo.\\

$\widehat{\beta_{1}} = 0.55817$ . Correspondiente a la estimación de la pendiente, indica que por cada aumento unitario de los ingresos, el consumo se aumenta en 0.55817 unidades.
 
Otro indicador importante en el análisis corresponde al coeficiente de determinación ($R^{2}$), que en este caso tiene un valor de $0.8447$ e indica que el modelo explica un $84.47$\% de la variacion o del comportamiento de la variable $Y$


\newpage 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
\textcolor{col3}{\bf \large Ejemplo 2:}  Un fabricante de equipos de aire acondicionado tiene problemas en la etapa de montaje, debido principalmente a la falta de una biela, pues debido a su peso no satisface las especificaciones establecidas para el producto por sobrepeso. Para reducir este costo, el fabricante estima que estudiando  la relación entre el peso de la barra en bruto y su peso final, es probable encontrar una relación entre ellas y de esta manera reducir el problema mediante la detección de elementos que probablemente no cumplan con las condiciones exigidas para el producto. \\
	
En el estudio se midieron un total de 25 (x, y) pares de barras, siendo $X$ el peso de la pieza colada en espera de ser procesada (materia prima) y $Y$ el peso de una biela terminada que forma parte del producto final. Se requiere estimar un modelo que permita predecir el valores del peso futuro de la  biela terminada como función del peso bruto del bloque de metal. \\
	
Los datos recogidos se muestran a continuación: \\
	
	Antes de realizar la estimación debemos visualizar mediante un diagrama de dispersión la relación lineal entre las variables. \\
	
	\begin{center}
		\begin{tabular}{ccc|ccc}
			\hline
			barra  & peso   & peso & barra  & peso   & peso \\
			Numero & bruto  & final& Numero & bruto  & final \\ 
			\hline
			1      & 2.745  & 2.080 & 14     & 2.635  & 1.990 \\
			2      & 2.700  & 2.045 & 15     & 2.630  & 1.990 \\
			3      & 2.690  & 2.050 & 16     & 2.625  &1.995 \\
			4      & 2.680  & 2.005 & 17     & 2.625  &1.985 \\ 
			5      & 2.675  & 2.035 & 18     & 2.620  &1.970 \\
			6      & 2.670  & 2.035 & 19     & 2.615  &1.985 \\
			7      & 2.665  & 2.020 & 20     & 2.615  &1.990 \\
			8      & 2.660  & 2.005 & 21     & 2.615  &1.995 \\
			9      & 2.655  & 2.010 & 22     & 2.615  & 1.990 \\
			10     & 2.655  & 2.000 & 23     & 2.590  &1.975 \\
			11     & 2.650  & 2.000 & 24     & 2.590  &1.995 \\
			12     & 2.650  & 2.005 & 25     & 2.565  &1.955 \\
			13     & 2.645  & 2.015 & &&\\
			\hline
		\end{tabular}
	\end{center}
	
	\begin{center}
		\includegraphics[scale=.7]{grafica_ej1}
	\end{center}
	
En la grafica se puede observar que existe una posible relación positiva entre las variables, la cual se puede representar a través de una linea recta con pendiente positiva.

Inicialmente se plantea el modelo formado por una variable respuesta ($y$) y una variable independiente ($x$), una variable aleatoria no observable o error ($u$)	y  dos coeficientes $\beta_{0}$ y $\beta_{1}$ y un conjunto de 25 observaciones. 
	
	
	$$y_{i}=\beta_{0}+\beta_{1}x_{i}+u_{i} \hspace{.7cm}\text{ con }\hspace{.2cm} i=1,2,3,\dots 25$$
	
Este modelo representa 25 igualdades que originan un sistema matricial $y= X \beta + u$, el cual se resuelve mendiante MCO con el siguiente resultado: 

	\begin{eqnarray*}
		b&=&(X^{T}X)^{-1}X^{T}y \\
		&=&\begin{bmatrix}
			{n}     &{\displaystyle\sum_{i=1}^{n} x_{i}}\\
			{\displaystyle\sum_{i=1}^{n} x_{i}}&{\displaystyle\sum_{i=1}^{n} x^{2}_{i}}\\
		\end{bmatrix}^{-1}
		\begin{bmatrix}
			{\displaystyle\sum_{i=1}^{n} y_{i}}\\
			{\displaystyle\sum_{i=1}^{n} y_{i}x_{i}}
		\end{bmatrix}      \\
		&=& 
		\begin{bmatrix}
			{25.00}  &{50.1200}\\
			{50.12}  &{100.4986}\\
		\end{bmatrix} ^{-1}
		\begin{bmatrix}
			{66.0800}\\
			{132.5007}\\
		\end{bmatrix}\\
		&=&
		\begin{bmatrix}
			{222.4160} &  {-110.9218}\\
			{-110.9218}&  { 55.3281}\\
		\end{bmatrix}
		\begin{bmatrix}
			{66.0800}\\
			{132.5007}\\
		\end{bmatrix}\\
		\begin{bmatrix}
			{b_{0}}\\
			{b_{1}}\\
		\end{bmatrix}&=&
		\begin{bmatrix}
			{0.03753679}\\
			{1.29971229}\\
		\end{bmatrix}
	\end{eqnarray*}
	
Mediante operaciones matriciales podemos obtener los estimadores de los coeficientes así:
	
	\begin{Box3}{} 
	\begin{verbatim} 
	y=c(2.745, 2.700, 2.690, 2.680, 2.675, 2.670, 2.665, 2.660, 
	    2.655, 2.655, 2.650, 2.650, 2.645, 2.635, 2.630, 2.625, 
	    2.625, 2.620, 2.615, 2.615, 2.615, 2.615, 2.590, 2.590, 
	    2.565)
	
	x=c(2.080, 2.045, 2.050, 2.005, 2.035, 2.035, 2.020, 2.005, 
	    2.010, 2.000, 2.000, 2.005, 2.015, 1.990, 1.990, 1.995,  
	    1.985, 1.970, 1.985, 1.990, 1.995, 1.990, 1.975, 1.995,  
	    1.955)
	
	unos=rep(1,25)
	X=matrix(c(unos,x),nrow=25)
	
	# Estimador MCO - Enfoque matricial
	b=solve(t(X)%*%X)%*%(t(X)%*%y)
	
	> b
	[,1]
	[1,] 0.03753679
	[2,] 1.29971229
	\end{verbatim}
	\end{Box3}
	
	
Finalmente el modelo estimado por MCO corresponde a la ecuación:

	$$ \widehat{y}= 0.03753679 + 1.29971229 \hspace{.2cm}x$$
	
	
	El signo esperado para el coeficiente $b_{1}$, está de acuerdo con los resultados encontrados. Como se ha mencionado anteriormente $b_{1}$ corresponde a la estimación de la pendiente de la recta de regresión muestral y su signo positivo indica que para un mayor peso de la barra antes de iniciar el proceso, el producto final tendrá un mayor peso. 
	
	Para realizar la estimacion de los coeficientes en R se debe correr el siguiente codigo:
	
	\begin{Box3}{}
	\begin{verbatim}
	regresion=lm(y~x)
	\end{verbatim}
	\end{Box3}
	Esta instrucción produce el siguiente resultado \\
	
\begin{Box3}{}
	\begin{verbatim}
		
	
	> # Estimación por MCO en R
	> summary(regresion)
	Call:
	lm(formula = y ~ x)

	Residuals:
	Min         1Q         Median   3Q        Max 
	-0.040463 -0.011457  0.002044  0.007534  0.036540

	Coefficients:
	Estimate   Std. Error  t value   Pr(>|t|)    
	(Intercept)  0.03754    0.23810     0.158     0.876    
	x            1.29971    0.11875    10.945     1.36e-10 ***
	---
	Signif.codes:0 *** 0.001 ** 0.01 * 0.05 . 0.1    1 

	Residual standard error: 0.01597 on 23 degrees of freedom
	Multiple R-squared: 0.8389,     Adjusted R-squared: 0.8319 
	F-statistic: 119.8 on 1 and 23 DF,  p-value: 1.356e-10

	\end{verbatim}	
\end{Box3}
	
	
En el listado para cada estimacion se presenta una prueba-t individual sobre la significancia de cada coeficiente asi:
	
	$Ho: \beta_{0} =0$ \\
	$Ha: \beta_{0} \neq 0$\\
	$EdeP : T = 0.158$\\
	valor-p:$  0.876$\\
	
	Indicando que no se rechaza la hipótesis nula y por tanto se asume que Ho es verdad , $\beta_{0}=0$. En este caso se recomienda que aunque la prueba de hipotesis indique que el intersecto es no significativo, se debe dejar en el modelo, pues el no incluirlo puede generar que la o las pendientes estimadas presenten un sesgo.
	\\
	
	Para el caso de la pendiente los resultados son los siguientes.
	
	$Ho: \beta_{1} =0$ \\
	$Ha: \beta_{1} \neq 0$\\
	$EdeP : T = 10.945$\\
	valor-p:$  0.0000$\\
	
	Lo cual ratifica que el modelo es valido y existe una relación lineal entre la variable de la barra inicial y el peso del producto terminado\\
	
	Además de los coeficientes estimados y de la validación de sus respectivas pruebas de hipótesis individuales, existe una medida para determinar el grado de explicación de la variable dependiente que es estimada por el modelo ($R^{2}=83.9\%$). En este caso el modelo explica el 83.9\% del comportamiento de $y$ o de su variación.\\
	
	
	Finalmente la salida reporta el valor del estadístico  F (119.8) y su valor-p asociado (0.0000), correspondiente al análisis de varianza o ANOVA , útil en análisis de regresión lineal múltiple. Para el caso del modelo de regresión lineal simple el análisis de varianza arroja los mismos resultados que la hipótesis individual para $\beta_{1}$\\
	
	
	\textcolor{col4}{\Large Validación de supuestos}\\
	
	El método de MCO, no requiere ningún supuesto de tipo estadístico, pues es un método que se basa en la optimización matemática. Sin embargo cuando se requiere realizar inferencia - construcción de intervalos de confianza o la realización de pruebas de hipótesis asociadas con los resultados obtenidos -  es necesario plantear y verificar el cumplimiento de supuestos estadístico sobre las variables que intervienen en el modelo. Algunos de ellos se realizan sobre las variables independientes y otros sobre la variable no observable error ($u$). Haremos énfasis sobre el cumplimiento de los siguientes supuestos, pues su violación genera estimaciones ineficientes y sesgados.
	
	\begin{itemize}
		\item[S1.] $E[u]=0$ . El modelo esta completo. Este supuesto garantiza que al hacer estimaciones el componente aleatorio desaparezca. Se valida mediante el calculo de la media de los residuales.
		\item[S2.] $V[u]=\sigma^{2}$. La varianza de los errores es constante. El supuesto de HOMOSCEDASTICIDAD o varianza constante hace que los estimadores MCO sean eficientes. Este supuesto se pude validar mediante la realización de otra regresión donde se toma como regresora una proxis de la varianza ($u^{2}$) y como variables independientes las mismas del modelo. Esta prueba se conoce como Test de White. En caso de que los coeficientes asociados con las variables independientes sean no significativos, este resultado será indicio de que la varianza es constante.
		
			$$\widehat{u}^{2} = \alpha_{0} + \alpha_{1} x + \epsilon $$
		
		\item[S3.] $Cor[u_{i},u_{i-1}]=0$. Los errores no están auto-correlacionados. El supuesto de no autocorrelación de errores se puede verificar mediante el estadístico Durbin-Watson. Para determinar si se rechaza o no el cumplimiento de este supuesto se debe construir una región de rechaz a partir de los datos obtenidos en la table de D-W
		\item[S4.] $u \sim N(0, \sigma^{2})$ . Para verificar este supuesto se puede construir una grafica de los residuales $qqnorm$ y una prueba de hipotesis de normalidad
	\end{itemize}
	
	
	Es necesario antes de utilizar el modelo, verificar el cumplimiento de los anteriores supuestos.\\

\newpage		
		\textcolor{col4}{\Large \bf Analisis de los resultados} 
		
		Listado 1. Supuesto de modelo completo\\
		\begin{Box3}{}	
		\begin{verbatim}
		> summary(residuos)
		Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
		-2.593880 -0.734629  0.131571 -0.002484  0.487185  2.335921 
		
		> t.test(residuos)
		One Sample t-test
		data:  residuos
		t = -0.012286, df = 24, p-value = 0.9903
		alternative hypothesis: true mean is not equal to 0
		95 percent confidence interval:
		-0istado\\.4196776  0.4147105
		sample estimates:
		mean of x 
		-0.00248355  
		\end{verbatim}
	\end{Box3}
		
		Por un lado el  promedio de los residuales es cercano a cero y por otro lado la prueba de hipótesis Ho: $\mu_{u}=0$ vs Ha: $\mu_{u} \neq 0$, no se rechaza, asumimos que el valor esperado de los errores es cero.
		
		
		Listado 2   Supuesto de no autocorrelación de errores \\	
		\begin{Box3}{}ç
		\begin{verbatim}
		# Es necesario cargar el paquete 
		library(lmtest)
		dwtest(y~x)
		> Durbin-Watson test
		> data:  y ~ x 
		> DW = 1.518, p-value = 0.07668
		> alternative hypothesis: true autocorrelation is greater than 0
		\end{verbatim} 
	   \end{Box3}
   
		Los resultados indican que no se cumple el supuesto de no autocorrelacion, los errores estan correlacionados
		
		
		Listado 3 Supuesto de normalidad de los errores\\
		\begin{Box3}{}
		\begin{verbatim}
		shapiro.test(residuos)
		> Shapiro-Wilk normality test
		> data:  residuos 
		> W = 0.9658, p-value = 0.5415
		\end{verbatim}
	    \end{Box3}	
		
		El valor-p correspondiente a la prueba de Shapiro-Wilk indica que los errores se pueden asumir como normales
		
		Listado 4\\	
		\begin{Box3}{}
		\begin{verbatim}
		# Supuesto de varianza constante de errores o homoscedasticidad
		summary(lm(residuos^2 ~ x))
		
		Residuals:
		Min      1Q  Median      3Q     Max 
		-1.1515 -0.8039 -0.4418 -0.0125  5.6504 
		
		Coefficients:
		Estimate Std. Error t value Pr(>|t|)
		(Intercept)   20.855     24.420   0.854    0.402
		x             -9.913     12.180  -0.814    0.424
		
		Residual standard error: 1.637 on 23 degrees of freedom
		Multiple R-squared:  0.028,	Adjusted R-squared:  -0.01426 
		F-statistic: 0.6625 on 1 and 23 DF,  p-value: 0.424
		\end{verbatim}
	  \end{Box3}
		
		Para la verificación del supuesto de homoscedasticidad indica que la variable independiente no esta relacionada con la varianza. Se puede suponer la varianza de los errores es constante.
%		
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		
		\newpage
	%	\textcolor{col4}{\Large \bf}  
        \begin{Box3}{ Código completo en R}
		\begin{verbatim}
		y=c(2.745, 2.700, 2.690, 2.680, 2.675, 2.670, 2.665, 2.660, 
		    2.655, 2.655, 2.650, 2.650, 2.645, 2.635, 2.630, 2.625, 
		    2.625, 2.620, 2.615, 2.615, 2.615, 2.615, 2.590, 2.590, 
		    2.565)
		x=c(2.080, 2.045, 2.050, 2.005, 2.035, 2.035, 2.020, 2.005, 
		    2.010, 2.000, 2.000, 2.005, 2.015, 1.990, 1.990, 1.995,  
		    1.985, 1.970, 1.985, 1.990, 1.995, 1.990, 1.975, 1.995,  
		    1.955)
		# Descriptivas
		summary(Y); summary(X)
		# Diagrama de dispersión X vs Y
		plot(y,x, main="Diagrama de dispersión",xlab="X:Peso inicial", 
		     ylab="Y:Peso final", pch=19)
		# Estimación por MCO
		regresion=lm(y~x)
		summary(regresion)
		confint(regresion)
		plot(y~x, xlab = "Peso inicial", ylab = "Peso final", pch=19)
		abline(regresion) 
		# Analisis de la varianza79po'
		anova(regresion)
		# Residuos
		residuos <- rstandard(regresion)
		valores.ajustados <- fitted(regresion)
		plot(valores.ajustados, residuos)
		# Diagnostico de normalidad
		qqnorm(residuos, main="Normalidad de los Residuales")
		qqline(residuos)
		plot(y,x, xlab = "peso-F", ylab = "peso-b")
		abline(y~x)
		# validación de supuestos del modelo
		residuos=rstandard(regresion)
		valores.ajustados <- fitted(regresion)
		plot(valores.ajustados, residuos)
		dwtest(y~x)
		bgtest(y~x)
		resettest(y~x , power=2, type="regressor")
		shapiro.test(residuos)
		\end{verbatim}
	   \end{Box3}
   
   
   \newpage		
   \textcolor{col4}{\Large \bf Regresión no lineal} \\
   \vspace{.5cm}
   
   
   Hasta ahora se ha presentado el modelo de regresión lineal mediante una función de linea recta $y= \beta_{0} + \beta_{1} x + u$, asumiendo que $y$ y $x$ son variables que se ajustan a este modelo.
   
   A continuación se presentan situaciones donde la relación no es lineal, teniendo que realizarse transformaciones en $x$, en $y$ o en ambas con el fin de lograr que el modelo resultante se lineal en los parámetros:
   
   Es necesario realizar un grafico de puntos que nos perita tener una primera impresión de la relación que puede existir entre las variables.  A continuación se presentan algunas relaciones que pueden ser linealizadas, permitiendo realizar una estimación por MCO. \\
   
   
  A continuación se presentan algunas formas de linealización que permiten la estimación de relaciones no lineales: 
\begin{center}
	 \begin{tabular}{lll}
\hline  	
 Función\hspace{3cm}. & Transformación\hspace{2cm}.& Forma lineal \\
\hline  
(1) Modelo exponencial &&\\
$Y = \alpha e^{\beta X}$ & $Y^{*}=ln(Y)$                            & $Y^{*} =ln(\alpha) + \beta X$\\  

&&\\
\hline 
(2) Modelo potencial &&\\
$Y = \alpha X^{\beta}$ & $Y^{*}=log(Y)$  & $Y^{*} =log(\alpha) + \beta X^{*}$\\ 
& $X^{*}=log(X)$  &                                    \\ 
&&\\
\hline
(3) Modelo reciproco &&\\
$Y = \alpha + \dfrac{\beta}{X} $ & $X^{*}=\dfrac{1}{X}$                        & $Y =\alpha + \beta X^{*}$\\ 
&&\\
\hline 
(4) Modelo hiperbólico &&\\
$Y=\dfrac{X}{\alpha X + \beta }$ &  	$Y^{*}=\dfrac{1}{Y}$ & $Y^{*} =\alpha + \beta X^{*}$\\
                                & $X^{*}=\dfrac{1}{X}$   &  \\
&&\\                                
\hline  
 \end{tabular}
\end{center}
   
 \begin{center}
 	 \includegraphics[scale=.7]{t1}  
 	 \includegraphics[scale=.7]{t2}  
 	 \includegraphics[scale=.7]{t3}  
 	 \includegraphics[scale=.7]{t4}  
 \end{center}
   
 El realizar el diagrama de dispersión y visualizar la relación entre $x$ y $y$, se puede intuir si existe una relación lineal o no entre las variables o si por el contrario se deben realizar transformaciones para mejorar la estimación por MCO.
 
\vspace{1cm} 
  \textcolor{col3}{\Large \bf Ejemplo} \\
 
 La buena previsión y el control de las actividades de preconstrucción conducen al uso más eficiente del tiempo y recursos en proyectos de construcción de autopistas. Los datos acerca de los costos de construcción (en miles de dólares) y las horas-persona de trabajo requeridas para varios proyectos se presentan en la tabla siguiente y fueron tomados del artículo “Forecasting Engineering Manpower Requirements for Highway Preconstruction Activities” (K. Persad, J. O’Connor, y K. Varghese, Journal of Management Engineering, 1995:41-47). Cada valor representa un promedio de algunos proyectos, y se han eliminado dos datos atípicos.(Problema 5. pp 545 Navidi(2006))\\
 
$y$: costos de construccion ( miles de dolares)\\
$x$: horas-persona trabajo \\
 
 \begin{Box3}{}
 	\begin{verbatim}
 		x=c(939,5796,289,283,138,2698,663,1069,5945,4159,1266,1481,4716)
 		y=c(251,4690,124,294,138,1085,345,355,5753,1377,802,945,2327)
 		plot(x,y)
 		grid()
 	\end{verbatim}	
 \end{Box3}

\begin{center}
	\includegraphics[scale=0.9]{ejeplo31}
\end{center}

En la grafica se visualiza una relación con una leve curvatura que puede hacer pensar que la relación entre las variables no es del todo lineal.  \\


Inicialmente se realiza la estimación lineal con el fin de poder comparar los otros modelos estimados.


\begin{Box3}{}
\begin{verbatim}
# Modelo lin-lin
modelo1=lm(y~x)
summary(modelo1) 

Call:
lm(formula = y ~ x)

Residuals:
Min      1Q  Median      3Q     Max 
-1496.6  -155.1   145.4   345.8  1510.8 

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept) -313.5213   320.1189  -0.979    0.348    
x              0.7663     0.1044   7.339 1.47e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 778 on 11 degrees of freedom
Multiple R-squared:  0.8304,	Adjusted R-squared:  0.815 
F-statistic: 53.86 on 1 and 11 DF,  p-value: 1.468e-05	
\end{verbatim}
\end{Box3}

\begin{Box3}{}
	\begin{verbatim}
# Modelo log-lin

modelo2=lm(log(y)~x)
summary(modelo2)

Call:
lm(formula = log(y) ~ x)

Residuals:
Min       1Q   Median       3Q      Max 
-0.65825 -0.30913 -0.03368  0.19121  0.71971 

Coefficients:
Estimate Std. Error t value Pr(>|t|)    
(Intercept) 5.320e+00  1.752e-01  30.359 5.86e-12 ***
x           5.478e-04  5.716e-05   9.583 1.13e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4259 on 11 degrees of freedom
Multiple R-squared:  0.893,	Adjusted R-squared:  0.8833 
F-statistic: 91.84 on 1 and 11 DF,  p-value: 1.129e-06
		
		
	\end{verbatim}
\end{Box3}

\begin{Box3}{}
	\begin{verbatim}
		# Modelo log -log
		modelo3=lm(log(y)~log(x))
		summary(modelo3)
		
		Call:
		lm(formula = log(y) ~ log(x))
		
		Residuals:
		Min       1Q   Median       3Q      Max 
		-0.74106 -0.33687 -0.03354  0.46597  0.65358 
		
		Coefficients:
		Estimate Std. Error t value Pr(>|t|)    
		(Intercept)  -0.1773     0.7945  -0.223    0.827    
		log(x)        0.9414     0.1095   8.597 3.27e-06 ***
		---
		Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
		
		Residual standard error: 0.4687 on 11 degrees of freedom
		Multiple R-squared:  0.8704,	Adjusted R-squared:  0.8587 
		F-statistic: 73.91 on 1 and 11 DF,  p-value: 3.274e-06
	\end{verbatim}
\end{Box3}

\begin{Box3}{}
	\begin{verbatim}
	# Modelo lin log
	modelo4=lm(y~log(x))
	summary(modelo4)	
	
	Call:
	lm(formula = y ~ log(x))
	
	Residuals:
	Min      1Q  Median      3Q     Max 
	-1338.5  -827.0  -526.8   535.6  2644.4 
	
	Coefficients:
	Estimate Std. Error t value Pr(>|t|)   
	(Intercept)  -6453.0     2112.2  -3.055  0.01095 * 
	log(x)        1100.3      291.1   3.779  0.00305 **
	---
	Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
	
	Residual standard error: 1246 on 11 degrees of freedom
	Multiple R-squared:  0.5649,	Adjusted R-squared:  0.5254 
	F-statistic: 14.28 on 1 and 11 DF,  p-value: 0.003051
	
	\end{verbatim}
\end{Box3}

\vspace{1cm} 
\textcolor{col4}{\Large \bf Intervalos de confianza de predicción} \\

En ocasiones es necesario realizar predicciones de los valores de $y$ para un determinado valor de $x$. 

La ecuación de predicción esta data por $\widehat{y}= b_{0}+ b_{1} x$. Esta ecuación permite estimar tanto $\widehat{E[y|x_{o}]}$, como $\widehat{y_{x_{o}}}$ . La primera corresponde al valor promedio para $y$ dado un valor promedio de $x_{o}$. La segunda a a un valor al valor de $y$ para un valor de $x_{o}$.

\vspace{1cm}
\textcolor{col4}{ \bf Intervalos de confianza de predicción para una sola respuesta $y_{o}$} \\
\begin{Box2}{Intervalo de confianza para $y_{o}$}
	$$
	\widehat{y_{o}} - t_{\alpha/2} s \sqrt{1+\dfrac{1}{n}+ \dfrac{(x_{o}-\bar{x})^{2}}{S_{xx}}}
	\hspace{.5cm}< y_{o} <\hspace{.5cm}
	\widehat{y_{o}} + t_{\alpha/2} s \sqrt{1+\dfrac{1}{n}+ \dfrac{(x_{o}-\bar{x})^{2}}{S_{xx}}}
	$$
	Donde: \\
	$S_{xx}=\displaystyle\sum_{i=1}^{n} (x_{i}-\bar{x})^{2}$	
	
\end{Box2}

Con respecto al Ejemplo 1, queremos estimar un intervalo de confianza para el consumo de un hogar que tenga unos ingresos de $x_{o}=18$ \\

En este caso el intervalo de confianza esta dado por: 



Retomando el Ejemplo 1 tenemos. En el supongamos que deseamos estimar un intervalo de confianza para el consumo ($\mu_{Y|x_{o}}$) para hogares con ingresos de $x_{o} = 18$.\\

\begin{Box3}{}
	\begin{verbatim} 	
		x=c(24.3,12.5,31.2,28,35.1,10.5,23.2,10,8.5,15.9,14.7,15)
		y=c(16.2,8.5,15,17,24.2,11.2,15,7.1,3.5,11.5,10.7,9.2)
		
		regresion=lm(y ~ x)
		summary(regresion)
		
		nuevo <- data.frame(x=18)
		predict(object=regresion, newdata=nuevo, interval="confidence", 
		        level=0.95)
		
		       fit          lwr          upr
		1 11.82497     10.36605     13.28388
		
		
	\end{verbatim}
\end{Box3}

\textcolor{col4}{ \bf Intervalos de confianza de predicción para la respuesta media $\mu_{Y|x_{o}}$} \\

\begin{Box2}{Intervalo de confianza para $\mu_{Y|x_{o}}$}
	$$
	\widehat{y_{o}} - t_{\alpha/2} s \sqrt{\dfrac{1}{n}+ \dfrac{(x_{o}-\bar{x})^{2}}{S_{xx}}}
	\hspace{.5cm}< \mu_{Y|x_{o}} < \hspace{.5cm}
	\widehat{y_{0}} + t_{\alpha/2} s \sqrt{\dfrac{1}{n}+ \dfrac{(x_{o}-\bar{x})^{2}}{S_{xx}}}
	$$
	Donde: \\
	$S_{xx}=\displaystyle\sum_{i=1}^{n} (x_{i}-\bar{x})^{2}$	
	
\end{Box2}

	\end{document}